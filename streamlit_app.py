import streamlit as st
import os
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate

# --- ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö Streamlit ---
st.set_page_config(page_title="AI Research Assistant", layout="wide")

# --- ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ OpenAI API Key ‡∏ú‡πà‡∏≤‡∏ô Sidebar ---
with st.sidebar:
    st.header("‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤")
    openai_api_key = st.text_input("‡∏Å‡∏£‡∏≠‡∏Å OpenAI API Key", type="password")
    os.environ["OPENAI_API_KEY"] = openai_api_key

# --- ‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏±‡∏ß‡∏Ç‡∏≠‡∏á‡πÅ‡∏≠‡∏õ‡∏û‡∏•‡∏¥‡πÄ‡∏Ñ‡∏ä‡∏±‡∏ô ---
st.title("üë®‚Äçüî¨ AI Research Assistant")
st.write("‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢ AI ‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡πÉ‡∏ô‡∏á‡∏≤‡∏ô‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì ‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÄ‡∏õ‡πÄ‡∏õ‡∏≠‡∏£‡πå‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢")

# --- ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á RAG Pipeline ---
@st.cache_resource(show_spinner="‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏á‡∏≤‡∏ô‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì...")
def load_and_index_papers(papers_path):
    """
    ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå PDF ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î, ‡∏ó‡∏≥ Indexing, ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á RAG chain
    """
    if not os.path.exists(papers_path) or not os.listdir(papers_path):
        return None

    # 1. ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ PDF ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    loader = PyPDFDirectoryLoader(papers_path)
    docs = loader.load()

    # 2. ‡∏ï‡∏±‡∏î‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡πÄ‡∏õ‡∏≠‡∏£‡πå‡∏¢‡∏≤‡∏ß‡πÜ)
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=2000,  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏ô‡∏≤‡∏î chunk ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏≤‡∏á‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏≤‡∏£
        chunk_overlap=400
    )
    splits = text_splitter.split_documents(docs)

    # 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á Vector Store (‡∏î‡∏±‡∏ä‡∏ô‡∏µ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤)
    embeddings = OpenAIEmbeddings(model="text-embedding-3-large") # ‡πÉ‡∏ä‡πâ embedding model ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô
    vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)

    # 4. ‡∏™‡∏£‡πâ‡∏≤‡∏á Prompt Template ‡∏ó‡∏µ‡πà‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏°‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô‡∏ß‡∏¥‡∏à‡∏±‡∏¢
    prompt = ChatPromptTemplate.from_template("""
    You are a world-class AI research assistant, specializing in the work of this specific author.
    Your task is to answer questions accurately and concisely based ONLY on the provided research papers.
    Synthesize information from multiple papers if the question requires it.
    When you answer, be precise and refer to methodologies, results, or discussions found in the context.

    Context from the papers:
    {context}

    Question: {input}

    Answer (in the same language as the question):""")

    # 5. ‡∏™‡∏£‡πâ‡∏≤‡∏á RAG Chain
    llm = ChatOpenAI(model="gpt-4o", temperature=0.1) # Temp ‡∏ï‡πà‡∏≥‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥
    document_chain = create_stuff_documents_chain(llm, prompt)
    retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 5} # ‡∏î‡∏∂‡∏á context ‡∏°‡∏≤ 5 ‡∏ä‡∏¥‡πâ‡∏ô ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô
    )
    retrieval_chain = create_retrieval_chain(retriever, document_chain)
    
    return retrieval_chain

# --- ‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ (User Interface) ---
if not openai_api_key:
    st.info("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏Å‡∏£‡∏≠‡∏Å OpenAI API Key ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÉ‡∏ô‡πÅ‡∏ñ‡∏ö‡∏î‡πâ‡∏≤‡∏ô‡∏Ç‡πâ‡∏≤‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô")
    st.stop()

# ‡πÇ‡∏´‡∏•‡∏î RAG pipeline
rag_chain = load_and_index_papers("./my_research_papers/")

if rag_chain is None:
    st.error("‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏á‡∏≤‡∏ô‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå 'my_research_papers' ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÑ‡∏ü‡∏•‡πå PDF ‡∏Å‡πà‡∏≠‡∏ô")
    st.stop()

st.success("AI ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏á‡∏≤‡∏ô‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÅ‡∏•‡πâ‡∏ß!")

# ‡∏™‡∏£‡πâ‡∏≤‡∏á session state ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏Å‡πá‡∏ö‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡πÅ‡∏ä‡∏ó
if "messages" not in st.session_state:
    st.session_state.messages = []

# ‡πÅ‡∏™‡∏î‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡πÅ‡∏ä‡∏ó
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ‡∏£‡∏±‡∏ö‡∏≠‡∏¥‡∏ô‡∏û‡∏∏‡∏ï‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
if prompt := st.chat_input("‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏á‡∏≤‡∏ô‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏Ñ‡∏ß‡πâ‡∏≤‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö..."):
            response = rag_chain.invoke({"input": prompt})
            answer = response["answer"]
            
            # ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
            st.markdown(answer)
            
            # ‡πÅ‡∏™‡∏î‡∏á‡πÅ‡∏´‡∏•‡πà‡∏á‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á
            with st.expander("üìö ‡∏î‡∏π‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á"):
                sources = {doc.metadata.get('source', 'N/A') for doc in response["context"]}
                st.write(f"‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ô‡∏µ‡πâ‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡πÄ‡∏õ‡πÄ‡∏õ‡∏≠‡∏£‡πå: {', '.join(sources)}")
                st.json([doc.metadata for doc in response["context"]])


    st.session_state.messages.append({"role": "assistant", "content": answer})
